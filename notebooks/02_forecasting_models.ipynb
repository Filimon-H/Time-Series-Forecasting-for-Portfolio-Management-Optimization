{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Task 2: Build Time Series Forecasting Models (TSLA)\n",
                "\n",
                "## Objective\n",
                "Develop, train, and evaluate time series forecasting models to predict Tesla's future stock prices.\n",
                "\n",
                "**Models:**\n",
                "- ARIMA/SARIMA (via pmdarima)\n",
                "- LSTM (Keras)\n",
                "\n",
                "**Metrics:**\n",
                "- MAE (Mean Absolute Error)\n",
                "- RMSE (Root Mean Squared Error)\n",
                "- MAPE (Mean Absolute Percentage Error)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Imports and Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "from statsmodels.tsa.arima.model import ARIMA\n",
                "from pmdarima import auto_arima\n",
                "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
                "from sklearn.preprocessing import MinMaxScaler\n",
                "\n",
                "import tensorflow as tf\n",
                "from tensorflow import keras\n",
                "\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "plt.style.use('seaborn-v0_8-whitegrid')\n",
                "pd.set_option('display.max_columns', None)\n",
                "pd.set_option('display.float_format', '{:.4f}'.format)\n",
                "\n",
                "print('TensorFlow version:', tf.__version__)\n",
                "print('Setup complete!')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Load Data (TSLA Adj Close)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "prices = pd.read_csv('../data/processed/adj_close_prices.csv', parse_dates=['Date'], index_col='Date')\n",
                "prices = prices.sort_index()\n",
                "\n",
                "tsla = prices['TSLA'].dropna().astype(float)\n",
                "\n",
                "print('TSLA series shape:', tsla.shape)\n",
                "print('Date range:', tsla.index.min().date(), 'to', tsla.index.max().date())\n",
                "print('Sample values:')\n",
                "tsla.tail()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Train/Test Split (Chronological)\n",
                "\n",
                "- **Train:** 2015-01-01 → 2024-12-31\n",
                "- **Test:** 2025-01-01 → end of data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "split_date = '2024-12-31'\n",
                "\n",
                "train = tsla.loc[:split_date].copy()\n",
                "test = tsla.loc[split_date:].iloc[1:].copy()  # Exclude split_date itself from test\n",
                "\n",
                "print(f'Train: {train.index.min().date()} → {train.index.max().date()} | n = {len(train)}')\n",
                "print(f'Test:  {test.index.min().date()} → {test.index.max().date()} | n = {len(test)}')\n",
                "\n",
                "# Sanity checks\n",
                "assert len(train) > 0 and len(test) > 0, 'Train or test is empty!'\n",
                "assert train.index.max() < test.index.min(), 'Data leakage: train/test overlap!'"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Metrics Utilities"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def calc_rmse(y_true, y_pred):\n",
                "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
                "\n",
                "def calc_mape(y_true, y_pred):\n",
                "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
                "    non_zero = y_true != 0\n",
                "    return np.mean(np.abs((y_true[non_zero] - y_pred[non_zero]) / y_true[non_zero])) * 100\n",
                "\n",
                "def evaluate(name, y_true, y_pred):\n",
                "    \"\"\"Compute metrics. Inputs can be arrays or Series.\"\"\"\n",
                "    y_true = np.array(y_true).flatten()\n",
                "    y_pred = np.array(y_pred).flatten()\n",
                "    \n",
                "    # Remove any NaN pairs\n",
                "    mask = ~(np.isnan(y_true) | np.isnan(y_pred))\n",
                "    y_true, y_pred = y_true[mask], y_pred[mask]\n",
                "    \n",
                "    if len(y_true) == 0:\n",
                "        return {'Model': name, 'MAE': np.nan, 'RMSE': np.nan, 'MAPE (%)': np.nan, 'N': 0}\n",
                "    \n",
                "    return {\n",
                "        'Model': name,\n",
                "        'MAE': mean_absolute_error(y_true, y_pred),\n",
                "        'RMSE': calc_rmse(y_true, y_pred),\n",
                "        'MAPE (%)': calc_mape(y_true, y_pred),\n",
                "        'N': len(y_true)\n",
                "    }\n",
                "\n",
                "print('Metrics functions defined.')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Baseline Model (Naive Persistence)\n",
                "\n",
                "Predict: tomorrow's price = today's price"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# For test period, naive prediction is the previous day's actual value\n",
                "naive_pred = test.shift(1).dropna()\n",
                "naive_actual = test.loc[naive_pred.index]\n",
                "\n",
                "baseline_metrics = evaluate('Naive (t-1)', naive_actual, naive_pred)\n",
                "print('Baseline metrics:', baseline_metrics)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. ARIMA Model\n",
                "\n",
                "Use `auto_arima` to find optimal (p,d,q), then use statsmodels ARIMA for forecasting."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print('Finding optimal ARIMA parameters with auto_arima...')\n",
                "print('This may take a few minutes...\\n')\n",
                "\n",
                "auto_model = auto_arima(\n",
                "    train.values,\n",
                "    start_p=0, max_p=5,\n",
                "    start_q=0, max_q=5,\n",
                "    d=None,  # Let it determine d\n",
                "    seasonal=False,\n",
                "    stepwise=True,\n",
                "    suppress_warnings=True,\n",
                "    error_action='ignore',\n",
                "    trace=True,\n",
                "    n_fits=50\n",
                ")\n",
                "\n",
                "best_order = auto_model.order\n",
                "print(f'\\nBest ARIMA order: {best_order}')\n",
                "print(auto_model.summary())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Fit ARIMA using statsmodels for more control\n",
                "print(f'Fitting ARIMA{best_order} on training data...')\n",
                "\n",
                "arima_model = ARIMA(train.values, order=best_order)\n",
                "arima_fit = arima_model.fit()\n",
                "\n",
                "print('Model fitted successfully!')\n",
                "print(arima_fit.summary())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Forecast for test period\n",
                "n_forecast = len(test)\n",
                "forecast_result = arima_fit.get_forecast(steps=n_forecast)\n",
                "arima_pred_values = forecast_result.predicted_mean\n",
                "\n",
                "# Create Series with proper index\n",
                "arima_pred = pd.Series(arima_pred_values, index=test.index, name='ARIMA_Pred')\n",
                "\n",
                "print(f'ARIMA predictions shape: {arima_pred.shape}')\n",
                "print(f'NaN count in predictions: {arima_pred.isna().sum()}')\n",
                "print('\\nFirst few predictions:')\n",
                "print(arima_pred.head())\n",
                "print('\\nActual test values:')\n",
                "print(test.head())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "arima_metrics = evaluate('ARIMA', test.values, arima_pred.values)\n",
                "print('ARIMA metrics:', arima_metrics)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. LSTM Model\n",
                "\n",
                "Steps:\n",
                "1. Scale data (fit on train only)\n",
                "2. Create sequences with lookback window\n",
                "3. Train LSTM\n",
                "4. Predict on test (one-step-ahead using true history)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "LOOKBACK = 60\n",
                "\n",
                "# Scale data\n",
                "scaler = MinMaxScaler(feature_range=(0, 1))\n",
                "train_scaled = scaler.fit_transform(train.values.reshape(-1, 1))\n",
                "\n",
                "print(f'Train scaled shape: {train_scaled.shape}')\n",
                "print(f'Scaler data min: {scaler.data_min_[0]:.2f}, max: {scaler.data_max_[0]:.2f}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def create_sequences(data, lookback):\n",
                "    \"\"\"Create sequences for LSTM: X[i] = data[i-lookback:i], y[i] = data[i]\"\"\"\n",
                "    X, y = [], []\n",
                "    for i in range(lookback, len(data)):\n",
                "        X.append(data[i-lookback:i, 0])\n",
                "        y.append(data[i, 0])\n",
                "    return np.array(X), np.array(y)\n",
                "\n",
                "X_train, y_train = create_sequences(train_scaled, LOOKBACK)\n",
                "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
                "\n",
                "# Validation split (last 10% of training sequences)\n",
                "val_size = max(1, int(0.1 * len(X_train)))\n",
                "X_tr, X_val = X_train[:-val_size], X_train[-val_size:]\n",
                "y_tr, y_val = y_train[:-val_size], y_train[-val_size:]\n",
                "\n",
                "print(f'X_train shape: {X_train.shape}')\n",
                "print(f'Training: {X_tr.shape}, Validation: {X_val.shape}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Build LSTM model\n",
                "keras.backend.clear_session()\n",
                "\n",
                "lstm_model = keras.Sequential([\n",
                "    keras.layers.Input(shape=(LOOKBACK, 1)),\n",
                "    keras.layers.LSTM(64, return_sequences=True),\n",
                "    keras.layers.Dropout(0.2),\n",
                "    keras.layers.LSTM(32),\n",
                "    keras.layers.Dropout(0.2),\n",
                "    keras.layers.Dense(1)\n",
                "])\n",
                "\n",
                "lstm_model.compile(optimizer='adam', loss='mse')\n",
                "lstm_model.summary()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train with early stopping\n",
                "early_stop = keras.callbacks.EarlyStopping(\n",
                "    monitor='val_loss',\n",
                "    patience=10,\n",
                "    restore_best_weights=True\n",
                ")\n",
                "\n",
                "history = lstm_model.fit(\n",
                "    X_tr, y_tr,\n",
                "    validation_data=(X_val, y_val),\n",
                "    epochs=50,\n",
                "    batch_size=32,\n",
                "    callbacks=[early_stop],\n",
                "    verbose=1\n",
                ")\n",
                "\n",
                "print(f'\\nTraining stopped at epoch {len(history.history[\"loss\"])}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot training curves\n",
                "plt.figure(figsize=(10, 4))\n",
                "plt.plot(history.history['loss'], label='Train Loss')\n",
                "plt.plot(history.history['val_loss'], label='Val Loss')\n",
                "plt.title('LSTM Training Curve')\n",
                "plt.xlabel('Epoch')\n",
                "plt.ylabel('MSE Loss')\n",
                "plt.legend()\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# One-step-ahead predictions on test set\n",
                "# For each test day, use the previous LOOKBACK days (from actual data) to predict\n",
                "\n",
                "full_data = pd.concat([train, test])\n",
                "full_scaled = scaler.transform(full_data.values.reshape(-1, 1))\n",
                "\n",
                "lstm_preds = []\n",
                "lstm_dates = []\n",
                "\n",
                "train_len = len(train)\n",
                "for i in range(train_len, len(full_data)):\n",
                "    # Get lookback window\n",
                "    start_idx = i - LOOKBACK\n",
                "    if start_idx < 0:\n",
                "        continue\n",
                "    \n",
                "    window = full_scaled[start_idx:i, 0]\n",
                "    X_input = window.reshape(1, LOOKBACK, 1)\n",
                "    \n",
                "    pred_scaled = lstm_model.predict(X_input, verbose=0)[0, 0]\n",
                "    lstm_preds.append(pred_scaled)\n",
                "    lstm_dates.append(full_data.index[i])\n",
                "\n",
                "# Inverse transform predictions\n",
                "lstm_pred_values = scaler.inverse_transform(np.array(lstm_preds).reshape(-1, 1)).flatten()\n",
                "lstm_pred = pd.Series(lstm_pred_values, index=pd.DatetimeIndex(lstm_dates), name='LSTM_Pred')\n",
                "\n",
                "print(f'LSTM predictions: {len(lstm_pred)}')\n",
                "print(lstm_pred.head())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Align with test for evaluation\n",
                "lstm_test_actual = test.loc[lstm_pred.index]\n",
                "\n",
                "lstm_metrics = evaluate('LSTM', lstm_test_actual.values, lstm_pred.values)\n",
                "print('LSTM metrics:', lstm_metrics)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Model Comparison"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "results = pd.DataFrame([baseline_metrics, arima_metrics, lstm_metrics])\n",
                "results = results.sort_values(by='RMSE').reset_index(drop=True)\n",
                "\n",
                "print('=' * 60)\n",
                "print('MODEL COMPARISON (sorted by RMSE)')\n",
                "print('=' * 60)\n",
                "results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualization: Test period predictions\n",
                "fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
                "\n",
                "# Full view\n",
                "ax1 = axes[0]\n",
                "ax1.plot(train.index, train.values, label='Train', linewidth=1, alpha=0.7)\n",
                "ax1.plot(test.index, test.values, label='Test (Actual)', linewidth=1.5, color='black')\n",
                "ax1.plot(arima_pred.index, arima_pred.values, label='ARIMA', linewidth=1.5, linestyle='--')\n",
                "ax1.plot(lstm_pred.index, lstm_pred.values, label='LSTM', linewidth=1.5, linestyle='--')\n",
                "ax1.set_title('TSLA Price: Full History + Forecasts')\n",
                "ax1.set_xlabel('Date')\n",
                "ax1.set_ylabel('Price ($)')\n",
                "ax1.legend()\n",
                "ax1.grid(True, alpha=0.3)\n",
                "\n",
                "# Zoomed to test period\n",
                "ax2 = axes[1]\n",
                "ax2.plot(test.index, test.values, label='Actual', linewidth=2, color='black', marker='o', markersize=3)\n",
                "ax2.plot(arima_pred.index, arima_pred.values, label='ARIMA', linewidth=1.5, linestyle='--', marker='s', markersize=3)\n",
                "ax2.plot(lstm_pred.index, lstm_pred.values, label='LSTM', linewidth=1.5, linestyle='--', marker='^', markersize=3)\n",
                "ax2.set_title('TSLA Price: Test Period (Zoomed)')\n",
                "ax2.set_xlabel('Date')\n",
                "ax2.set_ylabel('Price ($)')\n",
                "ax2.legend()\n",
                "ax2.grid(True, alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('../data/processed/model_comparison.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Model Selection & Discussion\n",
                "\n",
                "Based on the metrics above, select the best model for Task 3 forecasting."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "best_model_name = results.iloc[0]['Model']\n",
                "print('=' * 60)\n",
                "print(f'BEST MODEL (by RMSE): {best_model_name}')\n",
                "print('=' * 60)\n",
                "print(results.iloc[0])\n",
                "\n",
                "print('\\n--- Discussion ---')\n",
                "print('''\n",
                "Model Selection Rationale:\n",
                "\n",
                "1. ARIMA:\n",
                "   - Pros: Interpretable, provides confidence intervals natively\n",
                "   - Cons: Assumes linear relationships, may struggle with complex patterns\n",
                "\n",
                "2. LSTM:\n",
                "   - Pros: Can capture non-linear patterns and long-term dependencies\n",
                "   - Cons: Requires more data, harder to interpret, no native CI\n",
                "\n",
                "3. Naive Baseline:\n",
                "   - Useful benchmark; if models don't beat this, they add no value\n",
                "\n",
                "For Task 3 (future forecasting with confidence intervals), ARIMA is often \n",
                "preferred due to built-in uncertainty quantification.\n",
                "''')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. Save Artifacts"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import joblib\n",
                "\n",
                "os.makedirs('../data/processed/models', exist_ok=True)\n",
                "\n",
                "# Save LSTM model and scaler\n",
                "lstm_model.save('../data/processed/models/tsla_lstm.keras')\n",
                "joblib.dump(scaler, '../data/processed/models/tsla_scaler.joblib')\n",
                "\n",
                "# Save ARIMA info\n",
                "arima_info = {\n",
                "    'order': best_order,\n",
                "    'aic': arima_fit.aic,\n",
                "    'bic': arima_fit.bic\n",
                "}\n",
                "joblib.dump(arima_info, '../data/processed/models/tsla_arima_info.joblib')\n",
                "\n",
                "# Save results table\n",
                "results.to_csv('../data/processed/models/model_comparison.csv', index=False)\n",
                "\n",
                "print('Artifacts saved to data/processed/models/')\n",
                "print('  - tsla_lstm.keras')\n",
                "print('  - tsla_scaler.joblib')\n",
                "print('  - tsla_arima_info.joblib')\n",
                "print('  - model_comparison.csv')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print('\\n' + '=' * 60)\n",
                "print('TASK 2 COMPLETE')\n",
                "print('=' * 60)\n",
                "print('\\nDeliverables:')\n",
                "print('  ✓ Trained ARIMA model with documented parameters')\n",
                "print('  ✓ Trained LSTM model with documented architecture')\n",
                "print('  ✓ Model comparison table with MAE, RMSE, MAPE')\n",
                "print('  ✓ Discussion of model selection rationale')\n",
                "print('  ✓ Artifacts saved for Task 3')"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.11.14"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}